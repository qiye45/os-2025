# M6: GPT-2 并行推理 (gpt.c)
## 1. 背景

GPT (Generative Pre-trained Transformers) 系列模型开启了人工智能的新时代。从聊天机器人到多模态模型、具身智能，AI 将彻底改变人类社会运转的方式。GPT 这样的 "大语言模型" 本质上是一个函数 `f`，能够接收 "一串文本"，输出 "文本可能的下一个字符"。这个 "智能" 函数 `f` 具有惊人的应用场景，包括 "一切" 的 copilot (编程、操作系统使用、甚至是课堂学习)，和可以各种替代人类的智能 agent。"多模态" 模型甚至可以将文本、图像、声音、触觉等对齐到语言模型训练的向量上，实现与人类类似的 "智能"。

而 `f` 训练好之后，会在云端 (超级计算机) 或端侧 (手机、手表……) 部署，以 API 的形式提供服务。我们预见，AI 推理服务将会和水、电、云一样成为人类社会的重要基础设施。

## 2. 实验描述

### 2.1 总览

```bash
gpt [token]…
```

### 2.2 描述

使用 gpt2_124M 模型将输入的代表了一段文本的 token (整数) 序列进行补全，输出后续可能的 tokens (包含输入，总共有 `n=10` 个 tokens)，作为概率意义上 "可能" 的后续文本。

### 2.3 解释

GPT-2 是一个基于 Transformer 的早期模型，但奠定了 OpenAI 的技术基础。并行化是提高大规模模型计算效率的关键技术之一。gpt.c 实现了生成式人工智能的 "文本补全"，框架代码已经提供了从 llm.c 中裁剪出的完整的神经网络推理 (inference) 实现。下载 gpt2_124M.bin 到实验目录即可正常工作，实现文本的补全。与它交互，你可以感受到自然语言处理的飞速发展 (GPT-2 是 2019 年的模型，我们使用的是 124M 的小版本模型，能力与今天的模型无法匹敌，但依然能看到它的确可以生成 "合法" 的句子)。

⚠️ **你需要手动下载模型**
模型文件较大 (~500MB)，因此你需要手动下载到实验目录。这个文件不会被 git 追踪，也不会上传到服务器。

**下载地址：** https://huggingface.co/datasets/chrisdryden/llmcDatasets/tree/main

我们提供了一个 Python 的脚本 [`chat.py`](M6/gpt/c_version/chat.py:1)，可以直接输入一段文本，完成 tokenize，并且调用 gpt 命令行工具进行补全，没错，你真的在实验中使用了一个能生成文本的大语言模型！代码已经全部给大家准备好，并且可以正常工作了。在下面的例子里，数字序列 "31373 612" 就是 "Ladies and"。可以看到，语言模型的确为我们生成了可阅读的文本：

```bash
$ ./chat.py
Text to complete: Ladies and
gentlemen, we are gathered here today to honor the memory of our beloved friend, who passed away unexpectedly last year.
```

或
```bash
(base) ➜  gpt ./gpt   31373 612 338 635 281 
29226
29218
29236
29218
29232
```

没错，即便这么小的模型，单线程的实现在 CPU 上推理都非常吃力，更不必说之后 "超大" 的 GPT-3 (1750 亿参数)。神经网络推理优化是一个非常复杂的主题；在这里我们试着做出并行化的第一步：

### 🗒️ 实验要求：将 gpt.c 并行化

我们的 [`gpt.c`](M6/gpt/c_version/gpt.c:1) 是串行的代码 (但功能未变)，只能利用单个处理器。你需要找到代码中可并行 (并且有收益) 的部分，改造成并行，从而获得相应的性能提升。

在这个实验中，我们可以使用课堂上的线程库 ([`thread.h`](M6/gpt/c_version/thread.h:1) 和 [`thread-sync.h`](M6/gpt/c_version/thread-sync.h:1))，其中包括线程的创建和回收、互斥锁、信号量和条件变量。你可以选择你喜欢的机制进行同步和互斥。

## 3. 正确性标准

### 🗒️ 正确性 & Scalability

你并行化后的程序行为应当与我们给出的串行程序保持严格一致 (如果你希望动手 "玩一玩"，可以直接移步原 repo)。例如，我们输入 + 输出总共 `n=10` 个 tokens。你应该保持这个行为不变。

并行化带来的计算顺序调整可能会轻微影响整个神经网络的激活函数函数值，但只要最终输出的 token 序列 (理解为文本) 一致，我们就认为正确。相比于串行程序，在一个 `k` 个处理器的计算机上，除去模型加载时间，你应当在有较多轮推理时得到近似线性 (`k` 倍) 的加速比。

Online Judge 评测时，`k ≤ 4`。对于真实的神经网络训练/推导系统，GPU 这类 SIMT 的大规模并行处理器在能耗比上相比 CPU 有巨大的优势。回顾课程中讲解 SIMT 时，一个线程束共享一个 Program Counter，控制多个线程 "同步" 执行指令。此时，针对大矩阵、向量的 load/store，一个线程束就会生成一个非常长的内存 load (coalesced memory access)。GPU 也为这样的内存访问模式做出了特别的优化——相比 CPU 对 "逻辑程序" 设计的 memory hierarchy，具有高得多的电路比和能效比。

在这个实验中，你需要静态分配好线程 (例如 4 个 workers)，然后由这些线程完成计算任务。Online Judge 测试时的线程库与同学们实验代码中的完全一样。